{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lakehouse Modelling Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/overall_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark has some missing features like:\n",
    "    * Data storage infrastructure.\n",
    "    * ACID Transaction capabilities: (Automaticity, Consistency, Integrity, Durability)\n",
    "    * Metadata Catalog\n",
    "    * Cluster Manager\n",
    "    * Automation API's and Tools.\n",
    "* To overcome this missing features, we have got databricks.\n",
    "* There are other platforms as well like:\n",
    "    * Cloudera Hadoop Platform\n",
    "    * Amazon EMR\n",
    "    * Azure HDInsight\n",
    "    * Google Data Proc\n",
    "    * Databricks\n",
    "* Apache Spark was designed as an alternative to Hadoop's MapReduce. Databricks offers a cloud-based platform that leverages Apache Spark to build, deploy, and manage data applications efficiently.\n",
    "* it is the extension of map reduce.\n",
    "* Databricks Features:\n",
    "    * spark asCloude Native Technology: can use the spark on cloud. \n",
    "    * cloud storage ingestino: can integrate storage to spark securely.\n",
    "    * ACID transaction via Delta Lake Ingestino\n",
    "    * Unity Catelog for Metadata Management.\n",
    "    * cluster management\n",
    "    * Photon Query Engine: Query accelaration\n",
    "    * Notebook and Workspace\n",
    "    * Administrative control\n",
    "    * Optimize spark Runtime.\n",
    "    * Automation Tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Integration with Cloud Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/databrics_integration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks on Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create azure free account: https://azure.microsoft.com/en-in/free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Databricks Architecture\n",
    "\n",
    "![alt text](images/databricks_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create Databricks workspace??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Search for Databricks.\n",
    "* hit on create\n",
    "* fill out the details like:\n",
    "    * subscription\n",
    "        *   Resource group \n",
    "    * Workspace name\n",
    "    * Region\n",
    "    * Pricing tire:\n",
    "        * standard\n",
    "        * premium\n",
    "* fill out other  secrions such as network, advanced, tags, \n",
    "* then click review and create.\n",
    "* it will vaidate info and create workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cluster in Databricks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cluster_name\n",
    "* policy: \n",
    "    * limites users to create cluster with min or max settings. \n",
    "    * who can create cluster.\n",
    "    * limit users to create certain number of cluster.\n",
    "* Choose `Multi Node` or `Single node` cluster.\n",
    "* Databricks runtime version\n",
    "    * `use photon acceleration`: helps to accelerate execution of SQL or Dataframe.\n",
    "* Node type: machine configuration for worker and driver.\n",
    "* Termination time\n",
    "* Tags: user defined metadata about clusters. use them to identify in various reports.\n",
    "* Advance Option:\n",
    "    * default spark config and env variables\n",
    "    * logging: bydefault databricks will not store the cluster logs. set log locatio to store cluster logs.\n",
    "    * init script: specify cluster specific init script. Noting but the shell script to install or configure any library or requirements.\n",
    "* Create cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabs provided after cluster creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notebooks: number of notebook active\n",
    "* libraries: alows to add adition libraries.\n",
    "* event logs: logs created during the stand and the end time of the cluster.\n",
    "* Spark UI\n",
    "* Driver logs (stdout, stderr, log4j)\n",
    "* Metrices\n",
    "* Apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Databricks provides 3 Env in Notebook:\n",
    "* Database SQL\n",
    "* Databricks Data science and engineering\n",
    "* databricks Machine learning.\n",
    ">> THis three option will be available inside the workspace on the top left corner below databricks logo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Magic Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Magic command are used to run code written in different language.\n",
    "* write magic command on top of any cell to run code of different language.\n",
    "1. `%sql`: now we can write code in SQL in particular cell.\n",
    "2. `%scala`: write scala code \n",
    "3. `%python`: write python code.\n",
    "4. `%md`: write notes or documentation\n",
    "5. `%fs`: allows to run command file system commands (i.e. check list of directories `ls`). this dosnt offer complete lenux shell command.\n",
    "6. `%sh`: use to run shell commands.\n",
    "7. `%lsmagic`: get list of all magic commands that can be used with databricks notebook\n",
    "8. `%<command>?`: use question mark after every magic command to see documentation.\n",
    "9. `%<command>??`: look at the source code of magic command using double question mark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Utilities Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Utilities are the set of tools available in Python, R, and Scala notebooks that help users efficiently work with files, object storage, and secrets\n",
    "* Most commonly used are:\n",
    "    * fs\n",
    "    * notebooks\n",
    "    * widgets\n",
    "* other utilities:\n",
    "    * preview\n",
    "    * library\n",
    "    * jobs\n",
    "    * data\n",
    "    * credentials\n",
    "    * secrets\n",
    "    * meta\n",
    "* get info of all this utilities using: `dbutils.help()`\n",
    "* get documentatio for specific utility: `dbutils.<>UTILITY NAME>.help()` \n",
    "* get details to commads within a specific utility: eg: `dbutils.fs.help(cp)`\n",
    "\n",
    "#### USe `fs` Utility commands:\n",
    "* used to read and interact with DBFS file system.\n",
    "* bing values from filesystem to python variables and use them in your notebook\n",
    "* `dbutils.fs.ls()`\n",
    "* `dbutils.fs.mount()`: use to mount s3 bucket or any storage to the notebook.\n",
    "\n",
    "\n",
    "#### Use `notebook` utility command\n",
    "* allows you to chain your notebook. call one notebook from another.\n",
    "* this child notebook also return exit code to check whether the child notebook ran successfully.\n",
    "* this utility is used to run multiple notebook one after other.\n",
    "* we have 2 commands in notebook utility\n",
    "    * exit(): \n",
    "        * use to return value while exiting a notebook\n",
    "        * return an exit code while exiting form notebook\n",
    "        * command: `dbutils.notebook.exit(100)`\n",
    "    * run():\n",
    "        * takes 3 arguments (full path of child notebook, timeout in seconds, values to child notebook)\n",
    "        * command: `dbutils.notebook.run('/child_notebook', 10, 500)`\n",
    "        * helpful to check the values of child notebook.\n",
    "\n",
    "#### `widgets` utility\n",
    "* it allows you to add parameters to notebook \n",
    "* and set values to parameters while calling from parent notebook.\n",
    "* alllows you to add input arguments or parameters for a notebook.\n",
    "* allows to write parametrised code.\n",
    "* there are many methods within it.\n",
    "* most comman are `get()` and `text()`\n",
    "* use `text()`: \n",
    "    * used to pass arguments. \n",
    "    * works as a input box.\n",
    "    * it takes 3 arguments: (`name of input variable`, `default value (if in case no value is given within textbox)`, `label`)\n",
    "    * eg: `dbutils.widgets.text(\"msg_box\", \"\", \"Your Input Parameters\")`\n",
    "    * retrive value using get() method\n",
    "\n",
    "* use `get()`:\n",
    "    * used to get value in a python variable.\n",
    "    * eg: `my_msg = dbutils.widgets.get(\"msg_box\")`\n",
    "    * can also pass data to `msg_box` textbox using parent notebook.\n",
    "    * Commad from parent notebook: `dbutils.notebook.run('/child_notebook', 10, {'msg_box':'call from parent'})`\n",
    "    * this will get into textbox and used as input in child notebook.\n",
    "\n",
    "\n",
    "#### Advantage of using Utility command over Magic command\n",
    "* can use the output of utility commands in a python programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBFS (Databricks File system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a distributed file system abstraction on top of scalable cloud storage integrated into a databricks workspace and available on databricks cluster.\n",
    "* Alows you to access cloud storage as directories.\n",
    "* when we create workspace, databricks will launch control plane.\n",
    "* bydefault, databricks initially create a dbfs root directory/ storage directory and mount it to the databricks through DBFS.\n",
    "* user dont have total control on it. therefore its not recomended to store files within it.\n",
    "* to store data, we create seperate storage, and create root directoy there and mount that to databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access DBFS \n",
    "1. to see the list of directories in DBFS root storage: `%fs ls dbfs:/`\n",
    "2. Show contents of dbfs directory  inside  the root using following approaches\n",
    "    * With dbfs qualifier: `%fs ls dbfs:/databricks-datasets`\n",
    "    * Without dbfs qualifier: `%fs ls /databricks-datasets`\n",
    "\n",
    "3. show content of local file system: `%fs ls file:/`\n",
    "    * allows to access the local storage present on driver MACHINE.\n",
    "\n",
    "4. Show all mounted loations:`%fs mounts`\n",
    "    * storage mounted directories on root directores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating yout DBFS mount for data storage in Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create ADLS Gen2 Stroage account\n",
    "* Create storage container in your storage account.\n",
    "* Create Azure service principal and secret.\n",
    "* Gand access to service principle for storage container.\n",
    "* Mout Storage container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create ADLS Gen2 Storage account\n",
    "* Click \"Create a resource\" on your Azure portal home page\n",
    "* Search for \"Storage account\" and click the create button\n",
    "* Create a storage account using the following\n",
    "    * Choose an appropriate subscription\n",
    "    * Select an existing or Create a new Resource group\n",
    "    * Choose a unique storage account name\n",
    "    * Choose a region (Choose the same region where your Databricks service is created)\n",
    "    * Select performance tier (Standard tier is good enough for learning)\n",
    "    * Choose storage redundency (LRS is good enough for learning)\n",
    "    * Click Advanced button to move to the next step\n",
    "    * Select \"Enable hierarchical namespace\" on the Advanced tab\n",
    "    * Click \"Review\" button\n",
    "    * Click the \"Create\" button after reviewing your settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create storage container in your storage account\n",
    "* Go to your Azure storage account page\n",
    "* Select \"Containers\" from the left side menu\n",
    "* Click \"+ Container\" button from the top menu\n",
    "* Give a name to your containe (Ex dbfs-container)\n",
    "* Click the \"Create\" button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create Azure service principal and secret\n",
    "* Go to Azure Active Directory Service page in your Azure account (Azure Active Directory is now Microsoft Entra ID)\n",
    "* Select \"App registrations\" from the left side menu\n",
    "* Click (+ New registration) from the top menu\n",
    "* Give a name to your service principal (Ex databricks-app-principal)\n",
    "* Click the \"Register\" button\n",
    "* Service principal will be created and details will be shown on the service principal page\n",
    "* Copy \"Application (client) ID\" and \"Directory (tenant) ID\" values. You will need them later\n",
    "* Choose \"Certificates & secrets\" from the left menu\n",
    "* Click \"+ New client secret\" on the secrets page\n",
    "* Enter a description (Ex databricks-app-principal-secret)\n",
    "* Select an expiry (Ex 3 Months)\n",
    "* Click the \"Add\" button\n",
    "* Secret will be created and shown on the page\n",
    "* Copy the Secret value. You will need it later\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Grant access to service principal for storage account\n",
    "* Go to your storage account page\n",
    "* Click \"Access control (IAM)\" from the left menu\n",
    "* Click the \"+ Add\" button and choose \"Add role assignment\"\n",
    "* Search for \"Storage Blob Data Contributor\" role and select it\n",
    "* Click \"Next\" button\n",
    "* Click the \"+ Select members\"\n",
    "* Search for your Databricks service principal (Ex databricks-app-principal) and select it\n",
    "* Clcik \"Select\" button\n",
    "* Click \"Review + assign\" button twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical:\n",
    "\n",
    "* look at the notebook for `ADLS Gen2 storage to DBFS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unity Catelog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It a metadata storage for storing all metadata of project.\n",
    "* the first level of object that we can create in unity catelog is `catalog`.\n",
    "* Unity catelog is the complete solution for metadata management, user management, find grain access control for entire project and for entire envs from one centalized instance.\n",
    "* can create multiple catelog for multiple envs.\n",
    "* you can also have one unity catelog for multiple projects.\n",
    "    >> * Unity Catelog give user total control, to `grant permission` and `level of permission`\n",
    "\n",
    "* ![alt text](images/unity_catelog.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* `Unity Catelog`:\n",
    "\n",
    "    * `Location`:\n",
    "        * Locations in Unity Catalog represent external storage paths where data resides. They provide a way to reference external data storage in a structured and managed way within Databricks.\n",
    "    * `Storage Credentials`:\n",
    "        * Storage credentials in Unity Catalog are configurations that define how Databricks can access external storage systems. They typically include the necessary authentication information, such as access keys or IAM roles.\n",
    "    * `Catelog`\n",
    "        * `Schema`\n",
    "            * `Tables`:\n",
    "                * Tables in Unity Catalog are structured datasets organized in rows and columns, similar to tables in traditional databases. They are typically used for storing and querying structured data.\n",
    "                * The Schema is well defined.\n",
    "                * Best for structured data with a defined schema, supporting SQL queries, analytics, and reporting. Provides fine-grained access control and ACID transactions.\n",
    "                * Tables can be of 2 Types\n",
    "                    * `Unmanaged`:\n",
    "                        * *Storage Location:* Stored in an external location specified by the user.\n",
    "                        * *Creation:* User specifies external storage location.\n",
    "                        * *Data Management:* User manages data files.\n",
    "                        * *Deletion:* Only metadata is deleted; data files remain.\n",
    "                        * *Use Case:* Best for persistent, shared datasets and external data management.\n",
    "                    * `Managed`:\n",
    "                        * *Storage Location:* Stored in Databricks' default storage location.\n",
    "                        * *Creation:* Databricks handles storage and management.\n",
    "                        * *Data Management:* Databricks manages metadata and data files.\n",
    "                        * *Deletion:* Both metadata and data files are deleted.\n",
    "                        * *Use Case:* Best for automated data management and temporary or intermediate data.\n",
    "\n",
    "            * `Volume`:\n",
    "                * Volumes in Unity Catalog are storage units that allow users to manage unstructured or semi-structured data, such as files and objects, in a hierarchical namespace similar to directories and files in a file system.\n",
    "                * Best for unstructured or semi-structured data, supporting various data formats and hierarchical storage. Ideal for raw data storage, data lakes, and data ingestion. Provides access control at the directory and file levels.\n",
    "\n",
    "            * `Views`:\n",
    "                * Definition: Virtual tables based on SQL query results.\n",
    "                * Usage: Simplify complex queries, provide data abstraction, and enhance security.\n",
    "                * Types: Standard views, materialized views.\n",
    "                * Creation: Defined using SQL queries.\n",
    "                * Management: No additional storage for standard views, periodic refresh for materialized views.\n",
    "            * `Function`\n",
    "                * Functions are reusable SQL expressions or blocks of code that perform specific operations on data. They can be used to encapsulate complex calculations, transformations, or data manipulations.\n",
    "                * Usage: Simplify repetitive tasks, encapsulate business logic.\n",
    "                * Types: Scalar functions, aggregate functions, table-valued functions.\n",
    "                * Creation: Defined using SQL or supported programming languages.\n",
    "                *   Management: Defined and registered in the database/catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Medallion Architecture using Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/medallion_architecture_using_databricks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The architecture in red box shows the implementation of Medallion architecture.\n",
    "* At the below we have the physical layer, that stores all the data, i.e. ADLS container.\n",
    "* Above that we have logical layer, that is used to show all the records.\n",
    "* above that we have Unity catelog, that manages all the access to the logical layer.\n",
    "* above that we have spark application that access the data through unity catelog.\n",
    "* and finaly we have developers writing code in Workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake and Delta Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark lags the ACID properties, to overcome this databricks integrated Open source Delta Lake with Spark that possess ACID Property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Delta Lake??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delta Lake is the open source storage framework.\n",
    "* it work as a mediator between the distributed compute engen and storage layer.\n",
    "* Features:\n",
    "    * ACID Transaction\n",
    "    * Deletes, Updates and Merge\n",
    "    * Schema Enforcement and evaluation.\n",
    "    * Data Versioniing and time travel.\n",
    "    * Streaming and batch unification.\n",
    "    * optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* There are Various ways to create Delta tables\n",
    "* the 3 most common ways to create delta talbles are:   \n",
    "    * *Approach 1:* create table (default file format in databricks is delta table format)\n",
    "        * >> if you want to change the format in which file will be stored, se comand `using json;` or `using parquet;` after specifying the table columns.\n",
    "        * create table if not exists <catelog.dbname.>(column datatype)using delta\n",
    "\n",
    "    * *Approach 2:* load dataframe onto data lake\n",
    "        * flight_time_df.write.format('delta').mode('append').saveAsTable('dev.demo_db.flight_time_tbl')\n",
    "\n",
    "    * *Approach 3:* Create a delta table using Delta Table Builder API\n",
    "        * import delta table object `from detal import delta table`\n",
    "\n",
    "\n",
    "\n",
    "                (DeltaTable.createOrReplace(spark)\n",
    "                            .tableName('catelog.db name.table name')\n",
    "                            .addColumn(\"column name\", \"datatype\")\n",
    "                            .execute()\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Share delta tables to external location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create external location\n",
    "* Share data in delta format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data from external table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to do so, follow the below steps:\n",
    "    * Create external location\n",
    "    * Read using dataframe API\n",
    "    * Create external table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
