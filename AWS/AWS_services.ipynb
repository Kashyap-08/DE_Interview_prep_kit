{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Its a serverless computing service.\n",
    "* provides ability to execut code without configuring the cluster, lambda do it automatically for you.\n",
    "* we dont actualy pay for specific hardware that used in EC2.\n",
    "* run code directly, without worring about servers.\n",
    "* lambda can autoscale\n",
    "* only pay for the instance that you run, not for whole hardware.\n",
    "* AWS Lambda enables you to run code in response to specific events, such as HTTP requests via Amazon API Gateway, modifications to objects in Amazon S3, or updates to tables in Amazon DynamoDB. \n",
    "* You can write your code in various languages supported by Lambda, including Python, Node.js, Java, C#, Go, and Ruby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it works\n",
    "\n",
    "* upload code\n",
    "* set trigger\n",
    "* Execute\n",
    "* Sscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evnet-Driven Data Processing\n",
    "* Real-Time file and data transformation\n",
    "* Event Handling\n",
    "* Automation and Scheduling task.\n",
    "* deploy web apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "\n",
    "* No Server Management\n",
    "* Automatic scaling\n",
    "* cost-effective\n",
    "* Event-Driven\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages\n",
    "\n",
    "* Cold Start Latency\n",
    "* Execution Limit: AWS Lambda imposes a maximum execution duration of 15 minutes per invocation.\n",
    "* Resource Limits: There are limits on the amount of memory (128 MB to 10 GB) and temporary disk space (512 MB) available to Lambda functions.\n",
    "* Debugging and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ITs a fully managed ETL service that makes it easy to prepare and load your data.\n",
    "* You can create and run ETL jobs to transform and move data to different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application\n",
    "\n",
    "\n",
    "* Data Catelog\n",
    "* Run ETL jobs\n",
    "* SErverless - pay only for the resource used by ETL and catelog operation.\n",
    "* Integration  with AWS services.\n",
    "* job scheduling and orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AWS managed Relational Database service.\n",
    "* RDS makes it easy to setup, manage and scale relational Database in cloud.\n",
    "* used for light weight task such as (SignIn to console)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "* Automatically managed.\n",
    "* Scalable\n",
    "* performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS RedShift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fully managed Data Warehousing service\n",
    "* designed for large scale data analysis.\n",
    "* enables you to run complex query on petabyte of data seemlesly.\n",
    "* parallel query execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "\n",
    "* Scale and performance\n",
    "* Cost Effective\n",
    "* Advance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Redshift is different form RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| RDS | RedShift |\n",
    "|---|---|\n",
    "| Designed for OLTP database | Designed for OLAP database |\n",
    "| Suitable for Frequent Read and write | Optimized for complex query for analysis |\n",
    "| Support Multiple Database SQL SErver, PostgreSQL, Oracle | Integrated with PostgreSQL |\n",
    "| row-based storage | column -based storage |\n",
    "| sclaes vertically, increases instance size | scales Horizontally, adds more nodes to cluster |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Aurora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Its a fully managed relational database engen.\n",
    "* Its designed to combile the performance of multiple databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS DynamoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Its a fully managed no-sql database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, Write and Download file from/to s3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* install boto3\n",
    "\n",
    "* create access key and secret access key\n",
    "\n",
    "* Creare s3 Object\n",
    "\n",
    "        s3 = boto3.resource(\n",
    "            service_name = 's3',\n",
    "            region_name = 'us-east-2',\n",
    "            aws_access_key_id = '',\n",
    "            aws_secret_access_key = '')\n",
    "\t\n",
    "\n",
    "* get all the buckets \n",
    "\n",
    "        for bucket in s3.buckets.all():\n",
    "            print(bucket)\n",
    "\t\n",
    "\t\n",
    "* write a df to s3\n",
    "\n",
    "    * create dataframe\n",
    "    \n",
    "        s3.Bucket('bucket_name').upload_file(Filename = 'file_name.csv')\n",
    "\n",
    "\n",
    "\n",
    "* read all the files within buket\n",
    "\n",
    "        for files in s3.Bucket('bucket_name').objects.all():\n",
    "            print(files)\n",
    "\n",
    "\n",
    "* read particular file in df \n",
    "\n",
    "        obj = s3.Bucket('bucket_name').Object(key='file_name.csv').get()\n",
    "\n",
    "        df = spark.read.csv(obj['body'], index = 0)\n",
    "\n",
    "\n",
    "* download file \n",
    "\n",
    "        s3.Bucket('bucket_name').download_file(key = 'file_name.csv', Filename = 'myfile.csv')\n",
    "\n",
    "\n",
    "* delete file\n",
    "\n",
    "        s3.Bucet('buket_name').delete_file('file_name.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
