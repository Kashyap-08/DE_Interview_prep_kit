{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is apache spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Its a open-source, `distributed computing system` designed for fast and general-purpose data processing on large-scale datasets.\n",
    "* OR\n",
    "* Apache spark is a `unified` computing engine and set of libraries for parallel data processing on computer cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Previously we used to have Database servers which stores structure data.\n",
    "* The DB where not capable of handlig semi structured data. and same time we received large amount of data\n",
    "* we started facing issue of Big Data\n",
    "    * Big Data are 3'Vs (\n",
    "        * Velocity\n",
    "        * Variety\n",
    "        * Volume\n",
    "    )\n",
    "    * When we get the large `volume` of data at high `velocity` with different `variety` of datasets then it is termed as `Big Data`\n",
    "\n",
    "* We got the problem of storing and processing this big data.\n",
    "* Here we have 2 options to solve this problem:\n",
    "    * `Monolithic approach`: (scale the system vertically, increase the size of our system by adding more hardware)\n",
    "    * `distribted Approach`: Horizontal scaling ( add several different parallel systems ) (feature: High availability. Economical, Horizontal Scaling)\n",
    "* to implement distributed apporach we got `Hadoop` followed by `Spark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Vs Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameters| Hadoop | Spark |\n",
    "|---|---|---|\n",
    "| performance | Hadoop slow then spark, because it writes data to disk and read it back again from disk to in-memory | Spark is faster then hadoop as it does all computation in memory.|\n",
    "| file processing | Build for batch data processing | Batch as well as streming data processing. |\n",
    "|ease of use | Difficult to write code in hadoop, hive was build to make it easy. | Easy to write and debug code. Spark provides low and high level APIs |\n",
    "| security | Uses Kerberos authentication present within YARN Resource manager and ACL authentication to enter into any netowork. it uses tokens to get the permission. | Dosn't have solid security feature. It uses HDFC for accessing data and uses YARN for resource negotiatin |\n",
    "|Fault tollerent | It creates replica of data parts in different partition. | Spark creates DAG. if in case RDD is lost, it will regenrate data from DAG, therefore its a bit time consuming|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/spark_ecosys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Blog: https://www.encora.com/insights/apache-spark-architecture\n",
    "\n",
    "\n",
    "\n",
    "![alt text](images/spark__architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apache Spark has a master-slave architecture that consists of a **Driver**, **Executors**, and a **Cluster Manager**.\n",
    "\n",
    "**Driver Program:**\n",
    "- The **Driver** is the central coordinator in a Spark application. It is responsible for:\n",
    "    - Defining the main logic of the Spark application.\n",
    "    - Creating the **SparkContext**, which serves as the entry point for Spark functionality.\n",
    "    - Converting transformations into a logical Directed Acyclic Graph (DAG).\n",
    "    - Submitting jobs to the cluster manager and distributing tasks among the executors.\n",
    "\n",
    "**Executors:**\n",
    "- Executors are worker nodes in the cluster responsible for:\n",
    "    - Executing tasks assigned by the driver.\n",
    "    - Storing data for the application in memory or disk storage.\n",
    "    - Sending results back to the driver.\n",
    "\n",
    "**Cluster Manager:**\n",
    "- The Cluster Manager oversees resource management and job scheduling. Spark can run on various cluster managers, such as:\n",
    "    - **Standalone Cluster Manager:** A simple built-in cluster manager.\n",
    "    - **Apache Mesos:** A general cluster manager that can run Hadoop MapReduce and other applications.\n",
    "    - **Hadoop YARN:** The resource management layer of Hadoop.\n",
    "    - **Kubernetes:** An open-source platform for automating deployment, scaling, and operations of application containers.\n",
    "\n",
    "\n",
    "1. **Job Submission:**\n",
    " - The user submits a Spark application using the SparkContext in the driver program.\n",
    "\n",
    "2. **DAG Construction:**\n",
    " - The driver builds a logical Directed Acyclic Graph (DAG) of stages representing transformations and actions.\n",
    "\n",
    "3. **Task Scheduling:**\n",
    " - The DAG is divided into smaller sets of tasks, which are then submitted to the cluster manager.\n",
    "\n",
    "4. **Task Execution:**\n",
    " - The cluster manager allocates resources and schedules the tasks on available executors.\n",
    " - Executors perform the tasks assigned to them, which may involve reading data from a data source, performing computations, and storing intermediate results.\n",
    "\n",
    "5. **Result Collection:**\n",
    " - Executors send the results of their computations back to the driver.\n",
    " - The driver program consolidates these results and performs any final actions required.\n",
    "\n",
    "\n",
    "In Spark, operations on data are categorized into **transformations** and **actions**:\n",
    "\n",
    "- **Transformations:**\n",
    " - These are operations that create a new RDD from an existing one. They are lazy, meaning they donâ€™t execute immediately but instead create a logical plan of execution.\n",
    " - Eg. `map()`, `filter()`\n",
    "\n",
    "- **Actions:**\n",
    " - These operations trigger the execution of the transformations and return a result to the driver program or write data to an external storage system.\n",
    " - Eg.`collect()`, `count()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Transformation and How many type of transformations do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformatinos\n",
    "\n",
    "* transformations are operations that are applied to Resilient Distributed Datasets (RDDs) to create new RDDs. Transformations are lazy, meaning they do not immediately compute their results but instead, build up a series of transformations to be applied when an action is triggered. This allows Spark to optimize the execution plan and efficiently manage resources.\n",
    "\n",
    "* There are 2 types of Transformations `Narrow` and `Wide`\n",
    "\n",
    "##### Narrow Transformation:\n",
    "* Transformatin that does not require data movement between partitions.\n",
    "* Transformations which are not dependent on other partition of the data. and which executes with only one partition.\n",
    "\n",
    "* Eg:\n",
    "    * filter()\n",
    "    * map()\n",
    "    * union()\n",
    "    * coalesce()\n",
    "    * select()\n",
    "\n",
    "##### Wide\n",
    "* transformations that rquires data movement between paritions.\n",
    "* Multiple partitions of the data is used to perfrom the operation.\n",
    "* Eg:\n",
    "    * repartition()\n",
    "    * groupByKey()\n",
    "    * reduceByKey()\n",
    "    * join()\n",
    "    * distinct()\n",
    "\n",
    "### Actions\n",
    "* actions triggeres the execution of the transofrmations.\n",
    "* each action creates a new job.\n",
    "* actios typically returns the result to driver program or writes the data to storage.\n",
    "* Eg:\n",
    "    * collect()\n",
    "    * count()\n",
    "    * first()\n",
    "    * save()\n",
    "    * foreach()\n",
    "    * reduct()\n",
    "    * read()\n",
    "    * mean()\n",
    "    * inferschema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens when we use group by or join in transformation??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* groupBy() and join() typically involves shuffling of data, which makes it a wide transformation.\n",
    "* groupByKey groups all the values for each key in the RDD. This operation involves the following steps:\n",
    "    * Partition the data based on key. i.e. all vlaues associated with the same key are moved to same partition.\n",
    "    * shuffle: data is moved across the partitions.\n",
    "    * grouping: Spark groups the values by their keys within each partition.\n",
    "* same goes with join() operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Jobs are created in spark??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for every action a new job is created.\n",
    "* if there are multiple actions triggered. there will be multiple job createed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGs and Lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when the trainsformations are written they are lazily evaluated. i.e. the code will not execure untile the action is triggered.\n",
    "* till then spark created `DAG` that will have the logicla plan for the exection of the transformations.\n",
    "* Dag will be created for each job when an action is hit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL Engine/ Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Code -> Catalyst Optimizer -> Java Byte code\n",
    "\n",
    "\n",
    "* Process involved in Catalyst Optimizer\n",
    "    >> Code -> Unresolved Logical Plan -> Analysis (Catalog) -> Resolved logical Plan -> Logical Optimization ->Catalyst optimizer ->  Optimized Logical Plan -> Physical Plan -> Best Physical Plan -> Final Code (Set of RDDs to run on different executors.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is catalyst optimizer | spark SQL engine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Its a query optimization framework\n",
    "* Its designed to automatically optimize the execution of SQL query\n",
    "* It converts the program to java byte code.\n",
    "* There are 2 types of optimization technique used by catalyst optimzer:\n",
    "    * Rule Based Optimizer:\n",
    "        * when submiting a peace of code, execution engin will look at predefined plans, which are used to design logical plan.\n",
    "        * It will bliendly follow the rules.\n",
    "    * Cost Based Optimizer:\n",
    "        * Based on available data it estimates the cost and best effective execution plan is decided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many phases are involved in spark SQL engine to convent a code into Java byte code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 4 Phases involved in spark SQL Engine\n",
    "    1. Analysis:\n",
    "        * The unresolved logical plan is analysed. \n",
    "        * the catelog contain metadata of the data on which the transformation and action is being performed. \n",
    "        * catelog checks whether do we have specifed data regarding the tranformation. if not found we get `AnalysisException`\n",
    "    2. Logical Optimization:\n",
    "        * It takes the unsolved paln from analysis phase and optimize it logically.\n",
    "    3. Physical Plan:\n",
    "        * There can be multiple physical plan for a logical plan.\n",
    "        * for all the physical plan the best physical plan is choosen.\n",
    "        * It used the cost based optimizer to choose the best plan.\n",
    "    4. Code Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is catalog?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It stores the metadata about the data for which the transformation and actions are perofrmed.\n",
    "* It cheks whether proper metadata is being used in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we get Analysis exception error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When the catalog does not find the specific column for the data process. it will throw an analysis exception.\n",
    "* See the below code for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, lit, floor, rand\n",
    "\n",
    "persons = [\"person_\" + str(i) for i in range(1, 16)]\n",
    "\n",
    "df = spark.createDataFrame(persons, 'string').toDF('persons')\n",
    "\n",
    "# Add random weights to each person\n",
    "df = df.withColumn(\"person_weight\", floor(rand() * 50 + 80))  # Weights between 50 and 100\n",
    "\n",
    "# Add a unique sequence number\n",
    "df = df.withColumn(\"sequence_number\", monotonically_increasing_id()+1)\n",
    "\n",
    "# Show the result\n",
    "df.show()\n",
    "\n",
    "\n",
    "df.select('person_name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error  \n",
    "\n",
    "---------------------------------------------------------------------------  \n",
    "AnalysisException                         Traceback (most recent call last)  \n",
    "<ipython-input-5-12ff14ea2eb3> in <cell line: 18>()  \n",
    "     16   \n",
    "     17   \n",
    "---> 18 df.select('person_name').show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is physical planning / spark plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logical plans are converted to multiple physical plan.\n",
    "* physical plan are the exact execution steps, including specifics about data distribution, join algorithms, and more.\n",
    "* Most efficient physical plan is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is spark SQL engine \"a compiler\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yes, it convertes the code into java byte code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whart is RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD stands for Reselient Destributed Dataset.\n",
    "* Its a datastructure \n",
    "* and also a low level api\n",
    "* designed to handle distributed data processing\n",
    "\n",
    "\n",
    "#### Features\n",
    "* Fault Tolerant: it stores all the DAGs for each RDD, in case if any RDD is lost, new RDD is created by looking at the DAGs.\n",
    "* Immutable: becuase of immutability feature, we can recover lost RDD\n",
    "* Partitioned.\n",
    "* Lazy_evaluated.\n",
    "* Comple-time type safety\n",
    "\n",
    "\n",
    "#### Disadvantages:\n",
    "* No built-in optimization\n",
    "* more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session Vs Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spark context: \n",
    "\n",
    "* spark context is the entry point to the spark and defined in org.apache.spark package since 1.0 version .\n",
    "* It is use to create spark RDD.\n",
    "* if we want to use additional features of spark like sqlcontext, streamingcontext ect, we require to create seperate  context objects.\n",
    "* spark shell provides 'sc' variable by default for spark Context object.\n",
    "\n",
    "#### sparkSession:\n",
    "* spark Session was introduced in version 2.0 and defined in org.apache.spark.sql.SparkSession package\n",
    "* It's also a entry point to the spark.\n",
    "* all context object are encapsulated within it.\n",
    "* spark shell provides 'spark' variable by default for sparkSession object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job, Stages and Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are Job, stages and task in apache spark??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Application:\n",
    "    * whenever we hit a spark submit command, that created one applicatoin and that runs on the cluster.\n",
    "    * One application can have multiple Jobs.\n",
    "\n",
    "2. Jobs:\n",
    "    * job is created everytime the action is called.\n",
    "    * job can have multiple stages.\n",
    "\n",
    "3. Stages:\n",
    "    * Its the logical plan that being created.\n",
    "    * collection of stages is called as DAG.\n",
    "    * it determines the sequential execution of the program.\n",
    "\n",
    "4. Task:\n",
    "    * The logical plan are then converted into physical plan.\n",
    "    * The stages are then converted into different tasks. that will run on executors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many Job, stages and task will be created from the code snippet?\n",
    "\n",
    "![alt text](images/code1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of actions given:\n",
    "    * read()\n",
    "    * collect()\n",
    "\n",
    "* THere are 2 actions therefore 2 jobs will be created.\n",
    "\n",
    ">> * minimum one stage and one task will be created for each job\n",
    ">> * For each wide transformation new stage will be created.\n",
    ">> * Whenever there is a join or groupBy, bydefault we will be having 200 partitions.\n",
    ">> * Spark will create 2 jobs when we use `inferSchema =True` while reading the data.\n",
    ">>      * First Job: When you call spark.read.csv with inferSchema=True, Spark launches a job to sample the data and infer the schema. This involves tasks like reading the first few rows of the file or sampling a percentage of the file to determine the structure and data types of columns.\n",
    ">>      * Second Job: After the schema is inferred, Spark needs to read the data again with the inferred schema. This read operation involves another job because Spark now knows the data types and can parse the CSV file accordingly.\n",
    "\n",
    "* In the above code, we have 2 wide transformation defined `repartition()` and `groupBy()`.\n",
    "* For action `read()`\n",
    "\n",
    "| stage1 |\n",
    "|---|\n",
    "| read |\n",
    "|TASK|\n",
    "| task1\n",
    "\n",
    "\n",
    "* for action `collect()`\n",
    "\n",
    "\n",
    "| Stage2 | Stage3 | Stage4 |\n",
    "|---|---|---|\n",
    "| repartition | filter -> select | groupby |\n",
    "| TASK|||\n",
    "| stage 1 will have 1 task will create 2 partitions | this 2 partitions will be passed to stage2, which will create 2 task | groupby will create 200 partitions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repartition Vs Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Repartitioning??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repatition is used to evenly distribute the data within the partitions.\n",
    "* Mostly used when we encounter skewed data.\n",
    "* There can we situations where some executors completes there execution very quickly but some executors take time.\n",
    "* its because the executor may be processing partition with more value and data is more skewed in that partition.\n",
    "* Repatition can be used to increase or decrease partitions.\n",
    "* PROS:\n",
    "    * Evenly distribute data.\n",
    "    * helps to resolve skewness\n",
    "* Cons:\n",
    "    * Costly operatins as data shuffling is done.\n",
    "    * MOre use of resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Coalesce??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coalesce are generally used to reduce the number of partitions.\n",
    "* It will merge the small partition to make it a big partition.\n",
    "* No data shuffling is done.\n",
    "* less costly opoeration.\n",
    "* Coalesce can only reduce the partition and cannot increase partitions\n",
    "* PORS:\n",
    "    * Cost effective solution.\n",
    "    * fast and cost effective operation as no data shuffling is done.\n",
    "* Cons:\n",
    "    * Data is not evenly distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which One will you choose and why??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Depends on the senerio use it.\n",
    "\n",
    "* Coalesce:\n",
    "    * Imagine you have processed a large dataset and are about to write the final results to a file system like HDFS or S3. If your DataFrame has a large number of partitions, each partition will be written as a separate file. This can lead to a large number of small files, which can be inefficient for storage and further processing.\n",
    "\n",
    "* repartition:\n",
    "    * Scenario: Balancing Data Before a Join Operation\n",
    "\n",
    "    * Imagine you have two large DataFrames that you need to join on a key. If the data in these DataFrames is skewed (unevenly distributed across partitions), the join operation can become a bottleneck due to imbalanced workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition Vs Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| repartition | Coalesce |\n",
    "|---|---|\n",
    "| can increase or descrease partition | can only used to decrease partition |\n",
    "| more cosly operation | less costly operation |\n",
    "| shuffles the data | does not shuffles the data |\n",
    "| create equal distribution of data | can create imbalance data |\n",
    "| used when performing join on large datasets, if used skewed data it can be a bottleneck | can we used, when writing large dataset into a storage location. strong dataset with many number of partion can create number of part files, to reduce it we can use coalesce |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Join and Join Stragegy\n",
    "\n",
    "Video link: https://www.youtube.com/watch?v=ZkJowO83MZ0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How shuffling is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when we have data in 2 different executors and when we apply any joni or group by condition, the operation will create 200 partitions bydefault.\n",
    "* Now once the data is repartitioined, spark will bring same ID records into same executor,\n",
    "* the above scenerio is for executors that are in same worker node.\n",
    "* assume the data is in different worker nodes, then the shuffling operation becomes more costly.\n",
    "* In this way the shuffling is done.\n",
    "* after shuffling, Join or groupby operation is performed,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are join strategies in spark??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are different stategies are used to join or groupby the data\n",
    "* The strategies are mentioned below:\n",
    "    * Shuffle Sort-Merge Join `(Bydefauly join used by spark)` `(More CPU power is used)` `(time complexity = O(n log n))`\n",
    "    * Shuffle hash join `(More memory is used)` `(time complexity = O(1))`\n",
    "    * broadcast hash join `(time complexity = O(N))`\n",
    "    * Cartition JOin\n",
    "    * Broadcaset nested loop join `(time complexity = O(n^2))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Shuffle Sort-Merge Join\n",
    "    * Once the data shuffling is done.\n",
    "    * the data is sorted and then it will be merged with matching keys\n",
    "    * This strategy will use more `CPU power` \n",
    "    * It has the time complexity of `O (n log n)`\n",
    "    * bydefault, spark used this join strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Shuffle hash join:\n",
    "    * once shuffling is done, it will create a hash tabel of the `smallest table` based on keys.\n",
    "    * this hash keys are then compared with other tables keys and maching records are merged.\n",
    "    * This strategy will require `more memory`.\n",
    "    * if strategy used and suffecient memory not found, it will throw `MemoryOutOfBound Exception`\n",
    "    * it has the time complexity of `O(1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Broadcast hash join\n",
    "* > Interview questions:\n",
    "\n",
    "    * >>When do we need broadcast join??\n",
    "        * to avoide shuffling operation we use broadcast join.\n",
    "        * bydefault the size of broadcast table `10MB`. \n",
    "        * spark considers tables with less then `10MB` size as small tables and it performs a broadcast join.\n",
    "        * But spark also suggest that to decide the size of broadcast table depending upon the cluster configuration.\n",
    "        \n",
    "    * >> How does broadcast join works??\n",
    "        * the table which needed to be broadcast will be send to driver and driver will broadcast the table to each executors.\n",
    "\n",
    "    * >> Differenve between Broadcast hash join and Shuffle hash join??\n",
    "        * Shuffle hash join: \n",
    "            * Shuffling is done to execute the join which is costly operation.\n",
    "        * Broadcast hash join:\n",
    "            * Boradcast of table is done for join operation, which increases the execution speed.\n",
    "\n",
    "    * >> How can we get and sest broadcast size of table??\n",
    "        * TO get: spark.config.get('spark.sql.autoBroadcastJoinThreshold')\n",
    "        * To set: spark.config.set('spark.sql.autoBroadcastJoinThreshold', 30485760)  (`30485760 bytes` = `30 MB`)\n",
    "        * if switch off boradcast: spark.config.set('spark.sql.autoBroadcastJoinThreshold', -1)\n",
    "    * >> When broadcast table is not good  or where it can fail??\n",
    "        * when we try to broadcast larger table within the network, then there are chances of failure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Boradcase nested loop join\n",
    "    * sometimes we have situation where we need to do a cross join\n",
    "    * in such scenerio this strategy is very usefull.\n",
    "    * It is the most expensive join strategy\n",
    "    * It has the time complexity of `O(n^2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why joins are expensive??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* joins are expensive because they involves transfer of data between executors.\n",
    "* This transfer of data consumes lot of resources and eventually increases expenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between shuffle hash join and shuffle sort-merge join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Shuffle Sort-Merge JOin | Shuffle hash Join |\n",
    "|---|---|\n",
    "| Its the default Join Strategy | Need to set explicitly |\n",
    "| Sortes the shuffled data and then joins | Creates hash table from the smaller table and find the value in other table |\n",
    "| more CPU power is used | More memory is required. |\n",
    "| time complexity: `O (n log n)` | time complexity: `O(1)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accumulators are the global variable that recides on the Driver, and can only read by driver.\n",
    "* Executors can only update this variable, such as `add()` method.\n",
    "* Executors have `write_only` access, so they cannot view the data.\n",
    "* it is suitable to debugging for gathering metrixes.\n",
    "* Use cases: \n",
    "    * counting events\n",
    "    * debugging and Monitoring\n",
    "\n",
    "* Types of accumulators:\n",
    "    * Numeric Accumulators: Used for numerical aggregation operations like sum, count, etc.\n",
    "    * Custom Accumulators: Custom types of accumulators can be created by extending the AccumulatorV2 class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Memory Management | Driver Out of Memory exception\n",
    "\n",
    "https://www.youtube.com/watch?v=2Eb9mE7pfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Driver `java.lang.OutOfMemoryError`(OOM) in spark??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Its the type of Error that raised the memory overhead issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Do we get Driver Out of Memory (OOM) Exception??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* java.lang.OutOfMemoryError occurs when the collected data size from all executors are greater then the allocated memory in the Driver.\n",
    "* For Eg: If i have the driver memory of `1GB` and we arr trying to extract data from all executors which in total is more then 1GB, then the error occurs.\n",
    "\n",
    ">> NOTE: In driver we have 2 types of memory: \n",
    ">>* spark.driver.memory: that handles JVM Process\n",
    ">>* spark.driver.memoryOverhead: that handles non-JVM Process, such as maintating executors info, details about the application container etc. Any type of objects that get created are stored here.\n",
    ">>* Whenever we assign some amount of memory to Driver, 10% of the memory or 384 MB of memory, whichever is gratest will be assigned to `memoryOverhead`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Driver Overhead memory??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Memory that is within driver that handles `non-JVM` operations is called the Driver Overhead Memory\n",
    "* Whenever we assign some amount of memory to Driver, 10% of the memory or 384 MB of memory, whichever is gratest will be assigned to `memoryOverhead`.\n",
    "* Eg: if we assigned 1GB of driver memory, so from that 10% or 384 MB will be assigned to Driver memory overhead.\n",
    "    * Here 10% of 1GB is 100MB which is less then 384 MB, therefore 384 MB of the memory will be assigned to Driver Memory Overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Reason for `java.lang.OutOfMemoryError` ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The 3 common reasons for the above error:\n",
    "    1. Using `collect()` method:\n",
    "        * we must have used `collect()` method within code. \n",
    "        * `collect()` will pull all data from all executors and collect its in driver memory. \n",
    "    2. Boradcost join:\n",
    "        * There must be a `Broadcast` join used. when larger table is `Broadcasted`.\n",
    "    3. More Object is used in the process (When the Overhead memory is full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle `java.lang.OutOfMemoryError` ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check whether we are using collect() anywhere.\n",
    "* if Broadcast used, make sure the broadcast table is small enough.\n",
    "* make sure the Overhead memory is large enough to handle all the object and container info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Memory management | Exception out of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/executor%20struct.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Whenever we request a executor with specific memort and cores, we will be having `Overhead Memory` and `Executor Memory`.\n",
    "1. **Overhead Memory:**\n",
    "    * It is the 10% extra memory then requested memory.\n",
    "    * Eg: if we requested 10Gb of memory, spark will get 10% of 10GB = 1GB extra memory to Overhead Memory.\n",
    "    * It is used to store `container informaton (around 300-400MB)`, `Non-JVM Objects (pyspark application object)`\n",
    "2. **Executor Memory:**\n",
    "\n",
    "    * **Reserved Memory:**\n",
    "        * 300MB fix memory size is assigned to Reserve storage.\n",
    "        * it is used to store spark internal objects used by spark engen.\n",
    "        * >> NOTE: we need to create executor of more then 1.5 times of 300MB = 450 MB. if executor memory with less then 450 MB is created, the process will fail.\n",
    "    * **User Memory:**\n",
    "        * It is used to store UDF(User define function), User defined data structure or spark internal metadata.\n",
    "        * It stores RDD operations, like `aggrigate` -> Map partition transformation.\n",
    "        * Out of remaining, 40% memory is assigned to user storage.\n",
    "        * Eg: 40% of 9.7 GB = 3880 MB\n",
    "        * Memory distribution fraction between *User memory* and *Spark memory* can be changed using configuratio: `spark.memory.fraction`\n",
    "        * minimum 70:30 ration is suggested.\n",
    "        \n",
    "    * **Spark Memory:**\n",
    "        * It stores dataframe operations.\n",
    "        * Out of remaining, 60% memory is assigned to user storage.\n",
    "        * Eg: 60% of 9.7 GB = 5820 MB\n",
    "        * Memory distribution fraction between *User memory* and *Spark memory* can be changed using configuratio: `spark.memory.fraction`\n",
    "        * minimum 70:30 ration is suggested.\n",
    "        * It is devide into 2 sections:\n",
    "            * **Storage Memory Pool:**\n",
    "                * It is used to store intermediate stages of the task like joins.\n",
    "                * It stores catched data.\n",
    "                * Memory evacuation is done in LRU (least Recently used) fashion\n",
    "                * 50% of the spark memory is assigned here.\n",
    "                * Eg: 50% of 5820 MB = 2910 MB\n",
    "            * **Executor Memory Pool:**\n",
    "                * This memory will remail until the exevution of process. once process is completed, this memory become empty\n",
    "                * It stores object that is required during process execution.\n",
    "                * store hash table for hash aggrigation.\n",
    "                * *Shorte life span* -> cleaned after every execution.\n",
    "                * spilling in disk\n",
    "            \n",
    "            * TO manage the memory of `Storage Memory Pool` and `Executor Memory Pool` we have got `Static Memory Manager` and `Unified Memory Manager`.\n",
    "            * `Static Memory Manager` \n",
    "                * its the old depricated manager used before spark-1.6 version. \n",
    "                * In this the storage was statically defined and would not change after defined.\n",
    "                * Hard margin memory is defined.\n",
    "            * `Unified Memory Manager` \n",
    "                * its used after spark-1.6 version. \n",
    "                * In this, `Storage Memory Pool` and `Executor Memory Pool` can use each other memory if the memory is idea.\n",
    "                * There is no hard margin defined.\n",
    "                * *Assume*, we have done catching, it will be soted in `Storage Memory Pool`, but if `Storage Memory Pool` get full and `Executor Memory Pool` has some available memory, then it will catch the data in `Executor Memory Pool` as well.\n",
    "                * Now, if there are some exections needed to be done, they will run on `Executor Memory Pool` and if `Executor Memory Pool` get full, then it starts removing the occupied *catche* data stored in `Executor Memory Pool` on LRU basis (Least Recently used) and use those memory space.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we get Out Of Memory when data can be spilled to the disk??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* assume the `Executor Memory pool` and ` Storage memory pool` is full. now if we performm `join` operation, the data will get shuffled, and the aim of shuffling is to bring similar data in one partition, assume the size of similar data is more then the availabe memory space in `Executor Memory pool`, then the process get stuck in between and it will not spill the data to disk, here we get the choke point and we get OUt Of Memory Error.\n",
    "* To Resolve this isse, we need to use `repartition` or `salting`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How spark manages storage inside executors internally??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Executor has `Over head memory`and `Executor Memory` memory defined.\n",
    "* inside `Executor Memory` we have, `Reserved memory`, `spark memory` and `user memory`.\n",
    "* `spark memory` is further devided into `storage memory pool` and `executor memory pool`.\n",
    "* in this way executor handles storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How task are splitted in executors??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* each task will be run in each core parallelly.\n",
    "* and all this cores runs in `executor memoy pool`.\n",
    "* >> NOTE: Its recomended that to give 3-5 core per executors for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need overhead memory in executors??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* it stores container info and non-jvm objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## when do we get executor Out of Memoey Exception??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When there is no extra memory in `Executor memory pool` or `storage memory pool` or `executor Overhead` gets full or `user memory` is full\n",
    "* There can be multiple reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of memory managers in spark??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 2 main types of memory managers:\n",
    "    * Static memory manager.\n",
    "    * Unified memory manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Spark Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark is a command line tool, which is used to run spark code.\n",
    "* Spark submit makes the package of all the jar files and code and take it to the cluster to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you run your program on spark Cluster??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We run a program on cluster using spark submit.\n",
    "* Spark submit makes a package of code and jar files and send it to cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where is your spark cluster??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there are different type of cluster. \n",
    "    * Stand alone cluster.\n",
    "    * Local\n",
    "    * kubernetes (K8)\n",
    "    * Yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is deploy mode in spark submit??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 2 types of deployment modes.\n",
    "    * Client mode\n",
    "    * Cluster mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is master in Spark Submit??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `--master` option in spark-submit is used to specify the master URL for the cluster manager that Spark should connect to the cluster for running the application. \n",
    "* The master URL determines the deployment mode of the Spark application and specifies where the Spark driver should run.\n",
    "* There are different type of URL that can be used:\n",
    "    * local: `--master local[N]`\n",
    "        * Ex: `spark-submit --master local[*] --class com.example.MyApp my-app.jar`\n",
    "    * Standalone: `--master spark://<portnumber>`\n",
    "        * Ex: `spark-submit --master spark://spark-master-host:7077 --class com.example.MyApp my-app.jar`\n",
    "    * Apace Mesos: `--master mesos://<portnumber>`\n",
    "        * Ex: `spark-submit --master mesos://mesos-master-host:5050 --class com.example.MyApp my-app.jar`\n",
    "    * Hadoop YARN: `--master yarn`\n",
    "        * `--deploy-mode client`: The driver runs on the machine where spark-submit is executed.\n",
    "        * `--deploy-mode cluster`: The driver runs on one of the worker nodes in the cluster.\n",
    "        * Ex: `spark-submit --master yarn --deploy-mode cluster --class com.example.MyApp my-app.jar`\n",
    "    * Kubernetes: `--master k8s://https//<kubernetes-apiserver-host>:<kubernetes-apiserver-port>`\n",
    "        * Ex: `spark-submit --master k8s://https://k8s-apiserver-host:443 --class com.example.MyApp --conf spark.kubernetes.container.image=my-spark-image my-app.jar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you provide memory configuration and why did you uses this much memory??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* memory configuration can be provided as:\n",
    "    * --conf spark.driver.memory=4g \\\n",
    "    * --conf spark.executor.memory=8g \\\n",
    "    * --conf spark.executor.cores=4 \\\n",
    "    * --conf spark.executor.instances = 10 # `number of executors`\n",
    "\n",
    "* We require memory for smooth execution of application and the process should not throw out of memory error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to update configuration like broadcast join threshold, timeout, dynamic allocation??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* broadcast join threshold: `spark.sql.autoBroadcastJoinThreshold 1000` (bytes)\n",
    "* broadcast timeout: `spark.sql.broadcastTimeout 600` (seconds)\n",
    "* enable dynamic memory allocation: `spark.dynamicAllocation.enabled true`\n",
    "* dynamic memory allocation: \n",
    "    * `spark.dynamicAllocation.minExecutors 2`\n",
    "    * `spark.dynamicAllocation.maxExecutors 10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Mode and Edge Node in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is edge node??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Edge node works as a mediator between the cluster and the user.\n",
    "* Edge node is used to avoide a direct with the cluster.\n",
    "* ther must be situations where if direct access given to the developer, he may manuallly copy data to the cluster node and this data cant be replicated.\n",
    "* therefore, the code or data submission is done through edge node.\n",
    "* also edge node profide addition security. if anyone tries to login to edge node, it can authenticate the person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whata are the deployment modes in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 2 types of deployment modes:\n",
    "    1. Client Mode:\n",
    "        * if we submit code through edge node, the Driver will be created on edge node and all exectors will run of cluster.\n",
    "        * Here, if we shut down the edge node, all the executors will be lost as the driver was on local machine.\n",
    "        * also we will get a network latency, as the data transfer between the cluster and driver takes time.\n",
    "        * Advantage: We can see all the process running on the system and does not need to rely on cluster logs.\n",
    "    \n",
    "    2. Cluser Mode:\n",
    "        * The driver program will run on the cluster.\n",
    "        * even if we shut down the edge node, the process will continue its execution. \n",
    "        * if we want to recheck the status, we get a ApplicationID through which we can access the Spark UI and check status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need client and cluster mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Client Mode | Cluster Mode |\n",
    "|---|---|\n",
    "| Logs are generated on cluster machine. Its easy to debug. | Logs are generated on `stdout` or `stderr` files. its suitable for production workload. |\n",
    "| Network latency is high | Network latency is low |\n",
    "| High changes of Driver Out of Memory Error | Less changes of Driver Out of Memory Error |\n",
    "| Driver and Executors can be lost if the Edge server is closed. | Even if edge server closed, process can continue its execution |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Query Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is AQE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It was introduce after 3.0\n",
    "* AQE is the Dynamic Query Optimization technique.\n",
    "* AQE optimizes the plan at runtime\n",
    "* There are 4 main features of AQE:\n",
    "    \n",
    "1. `Dynamically coalesce shuffle partition:`\n",
    "    * Assume, you have a skewed data and you have perform a `groupBy` operation.\n",
    "    * Eg: Sales Dataset, with 80% sugar sales, and 20% other product sold.\n",
    "    * Here, firstly the data will be shuffled, and 200 partitions will be created,\n",
    "    * data with similar keys are moves to one partition.\n",
    "    * There will be scenerio where multiple empty partitions will be created.\n",
    "    * Here, with the static optimiation, we may have 1 task that will run for a longer time.\n",
    "    * But to avode such situation,AQE comes into picture. AQE enters when there is a `shuffling` of data.\n",
    "    * Here, AQE will `coalesce` the combine the partitions which has small amount of data within it.\n",
    "    * it will try to create a partition that is approximately equal data compared to the skewed partion size.\n",
    "    * if the coalesce partition is aprox equal to skewed partition. the process will take place.\n",
    "    * Here the AQE will combine the smaller partitions, and will remmove all the empty partitions as well.\n",
    "    * This will eventually reduces the number of core required to run the task. hence, save the usage of cores.\n",
    "    * ![alt text](images/AQE_coalesce.png)\n",
    "    * `Dynamically coalesce shuffle partition` feature will be enabled if we set `spark.sql.adaptive.enabled` and `spark.sql.adaptive.coalescePartitions.enabled` configurations are true.\n",
    "\n",
    "\n",
    "\n",
    "2. `Dynamically Optimizing skewed Join:` (config: `spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled`)\n",
    "    * Assume, even if coalescing the different partitons,  the new partition is less then the size of partition that has the skewed data.\n",
    "    * Then, AQE will use different technique.\n",
    "    * here, it will check if following rules are satesfied:\n",
    "        * Rule 1: if the skewed partitoin size is more then `256 MB`.\n",
    "        * Rule 2: if the partition size is 5 times more then the median.\n",
    "    * if both this condition satesfies, AQE will break the skewed data partition into smaller partitions, so that it will be approximately equal to the size of smaller data partition.\n",
    "    * ![alt text](images/rule_based_partition.png)\n",
    "\n",
    "3. `Dynamically Switch Join Strategy:`\n",
    "    * if the tables that are going to join has size more then 10 MB, then the AQE will not update the plan.\n",
    "    * But assume, if we have 2 dataframes that needed to be join df1 of size 10GB and df2 of size 1 GB.\n",
    "        * Assume we wrote some fiters thats bringins the df2 size to 7 MB,\n",
    "        * here we have a dataframe of size less then 10 MB,\n",
    "        * while shuffling the data `local Shuffle Reader` will read the data and \n",
    "        * If the AQE is enabled, it will optimize the final logical plan and apply `Broadcast join`, insted of `sort merge join`.\n",
    "        * Config to enable `local Shuffle Reader`: `spark.sql.adaptive.localShuffleReader.enabled`\n",
    "\n",
    "4. `Converting sort-merge join to shuffled hash join:`\n",
    "    * AQE converts sort-merge join to shuffled hash join when all post shuffle partitions are smaller than a threshold, \n",
    "    * the max threshold can see the config `spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need AQE??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benifits of AQE!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adaptive Query Execution (AQE) in Spark provides several key benefits that enhance performance and efficiency:\n",
    "\n",
    "    1. Dynamic Partition Pruning:\n",
    "        * AQE dynamically prunes partitions that are not required for a query. \n",
    "        * This reduces the amount of data shuffled and processed, leading to significant performance improvements, especially in large datasets with many partitions.\n",
    "\n",
    "    2. Optimized Join Strategies:\n",
    "        * AQE can dynamically choose the best join strategy based on runtime statistics. \n",
    "        * For example, it can switch from a sort-merge join to a broadcast join if it detects that one side of the join is small enough to be broadcasted, which can reduce the shuffle size and improve performance.\n",
    "\n",
    "    3. Handling Skewed Data:\n",
    "        * AQE helps in mitigating data skew by dynamically coalescing and splitting shuffle partitions based on the data distribution observed at runtime. \n",
    "        * This ensures a more balanced workload across tasks, preventing performance bottlenecks caused by skewed data.\n",
    "\n",
    "    4. Reduced Shuffle Partitions:\n",
    "        * AQE can reduce the number of shuffle partitions dynamically based on the actual data size, leading to better resource utilization and reduced overhead associated with managing many small partitions.\n",
    "\n",
    "    5. Automatic Re-optimization:\n",
    "        * AQE re-optimizes query plans based on runtime statistics. This allows Spark to adjust its execution strategy if the initial assumptions made by the query optimizer do not hold true during actual execution.\n",
    "\n",
    "    6. Improved Resource Utilization:\n",
    "        * By adapting the execution plan at runtime, AQE helps in better utilization of cluster resources. \n",
    "        * It reduces unnecessary computation and I/O, leading to more efficient use of CPU, memory, and disk.\n",
    "\n",
    "    7. Ease of Use:\n",
    "        * AQE automates many optimizations that would otherwise require manual tuning and expert knowledge. \n",
    "        * This makes it easier for users to achieve optimal performance without deep expertise in Spark internals.\n",
    "\n",
    "    8. Enhanced Query Performance:\n",
    "        * Overall, the combination of these optimizations leads to significant improvements in query performance, making Spark more efficient and responsive in handling large-scale data processing tasks.\n",
    "\n",
    "Interviewer: These benefits sound very advantageous. How does AQE compare to traditional query execution in Spark?\n",
    "\n",
    "Candidate: Traditional query execution in Spark relies on static optimization based on the logical plan generated during query compilation. It doesn't adapt to the actual data characteristics encountered at runtime. In contrast, AQE allows Spark to make intelligent adjustments based on real-time statistics, leading to more efficient and optimized query execution. This dynamic adaptation makes AQE superior in handling diverse and unpredictable workloads, providing better performance and resource efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cache and persisit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Cache??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Caching is the optimization technique.\n",
    "* it stores the intermediate result.\n",
    "* bydefault we get `MEMORY_AND_DASK` storage level. and this cant be changed.\n",
    "\n",
    "* Ex:\n",
    "\n",
    "        df = spark.read.csv('path')\n",
    "        df1 = some transformatinos\n",
    "        df2 = df.join(df1, col1 == co2, 'inner')\n",
    "        df3 =  some transformation\n",
    "        df4 = df.join(df3, col1 == col2, 'inner')\n",
    "        df4.show()\n",
    "\n",
    "    * In the above case, we are using df multiple times.\n",
    "    * we learned that RDD are immutable, it means always a new RDD is created.\n",
    "    * here when the executor completes the operation of calculating df, then it empty its storage and takes other tasks into memory.\n",
    "    * here, whenever we want to use df to join, spark recalculates the df using the DAG, which requires CPU.\n",
    "    * to avoide this problem catch the `df` so that we can use the data without recalculationg.\n",
    "    * cached `df` will be stored in `storage memory pool`. if `storage memory pool` get full and we are using `unified memory manager` then some partition will be stored in `executor memory pool` as well.\n",
    "    * `POINT TO NOTE:` whenever the data is cached, only the partition that are fully filled are stored and partially filled partitions are discarded, this lost partition data will be recalculated.\n",
    "    * to cache the data we just need to write `df.cache()`.\n",
    "    * to uncache data use command `df.unpersist()`.\n",
    "\n",
    ">> NOTE: whenever we do `show()` spark will cache only one partition and show() dosnt require all reocrds.  \n",
    ">> On the other hand, if we do `count()` spark will cache all the data within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use Catch and Persist??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cache is used when you have enough memory to store entire RDD.\n",
    "* when you want to use the default storage level, go with Cache.\n",
    "* but if you want more flexibility go with persist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When should we avoide caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When the DF is too small or easy to calculate, in such cases we shuld not use catching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to uncatch the data??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `df.unprsist()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between catche and persist??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Cache | Persist |\n",
    "|---|---|\n",
    "| It can only store data in `memory and disk only` | it provide more flexibility. |\n",
    "| go with cahce() when you want to use the default storage caching | use persist() when you want to cache data in disk as well . |\n",
    "| cache is a wrapper class built on persisit | persisit is the core class |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are different storage level in Spark??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| level| Space used |CPU time |\n",
    "|---|---|---|\n",
    "| MEMORY_ONLY | high | low |\n",
    "| MEMORY_ONLY_SER | Low  | high |\n",
    "| MEMORY_AND_DISK  | high  | medium |\n",
    "| MEMORY_AND_DISK_SER | low  | high |\n",
    "| DISK_ONLY  | low | high |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which Storage level to choose??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* its a part of experimentation.\n",
    "* firstly start with memory only and then move to memory and disk followed by disk only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Resouce Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Dynamic Resouce Allocation?? and why dynamic resource allocation was introduced??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In dynamic resource allocation, the resources can be reduced and increase as per requirements.\n",
    "* while using the static memory allocation, the fixed number of resources were allocated to a user.\n",
    "* even if the user is not using this resources, they will be reserved to the user and can't be used by other applications.\n",
    "* to avoide this problem, dynamic resource allocation was introduced. \n",
    "* We need to configure our spark applicatin to use dynamic resource allocation using config: `--config spark.dynamicResourceAllocation.enable = True`\n",
    "* While setting this config, we also need to specify the minimum and maximum executors to be used using congif:\n",
    "    * `--config spark.dynamicAllocation.minExecutors = 5` and `--config spark.dynamicAllocation.maxExecutors = 100`\n",
    "* when we set this, our application will have minimum of 5 executors but not less then this. and max upto 100 executors.\n",
    "* According the usage the resources are released.\n",
    "* NOTE: resource manage does not ask spark application to leave the resource if not used.\n",
    "* it is done automatically.\n",
    "* if the executor is idea for `60 sec` spark will release that executor and other application can use it.\n",
    "* even if the executors are released, an indepedent folder named `shuffle tracking` will be available that holds all the information about the previous shuffles, so that in case the application asks for the resources again, it does not have to recalculate everything.\n",
    "* `shuffle tracking` is available in every executor bydefault.\n",
    "* we can chnage this default idea time of executors using configuration.\n",
    "* Now if the application want the release application back, it will wait for `1s`, if it does not find its requires resources, it will ask resource manager to allocate resources.\n",
    "* Spark will ask resources in `two-fold` fashion.\n",
    "    * fiestly the spark will ask for `1` executor, if thats not suffecient it will double the request and ask for `2` executors and even thats not sufficient it will ask for `4` resources followed by `8` and then `16` and then `32` and so on, till the max executors reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Resource manager provides resources in Dynamic resource allocation??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Resource maanger does not goes to spark application to request the resourcs.\n",
    "* if the executors are idea for `60sec` spark will release resource.\n",
    "* if required it will ask them back in `two-fold` fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the resource allocation technique we have in spark??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are 2 techniques:\n",
    "    * Static Resource allocation\n",
    "    * Dynamic resource allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the challenges involved with dynamic resource allocation??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if at some point the applicatin releases its resources because of idea state and if it requires the resources back for critical execution, at that point if it does not get resources the process can fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to avoide dynamic resource allocation??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When you are running any critical application in PROD and its recemened not to use dnamic resouorce allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How spark will remove or add resources??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if the executors are idea for `60s` it will release the executor for other application.\n",
    "* if application want to retain its executors, it will wait for `1s` and if it does not find the required resources, it will request executors in `two-fold` fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration needed for dynamic resource allocation??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/spark_config.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Pruning / Dynamic Partition Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![alt text](images/code2.png)\n",
    "* in the above code, if the partition prunig is not applied, the spark will read all the data and then filter the where clause.\n",
    "* But when `partition pruning` is applied, the spark will firstly apply the `where` clause while reading the data and only read partition which has the specific data\n",
    "* The technique in which the `where` clause is pushed down wile reading the file is called as `Predicate Pushdown`\n",
    "\n",
    ">> `Point to NOTE:` Partition Pruning will only work when the data is partiioned on the specific key on which filter is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need Dynamic Partition Pruning??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![alt text](images/Code3.png)\n",
    "* In the above code we have 2 tables `orders` and `dates`.\n",
    "* here we are joining 2 DF and then filtering out data on the basis of dates columns.\n",
    "* In this case `partition pruning` will not work and spark will read all data, join them and then filter the data.\n",
    "* TO solve this problem, `Dynamic partition pruning` was introduced. \n",
    "* Dynamic Partition Pruning was introduced in Spark 3.0 and above.\n",
    "* Lest modify the above code:\n",
    "* ![alt text](images/code4.png)\n",
    "* The `Dynamic partition pruning` will firstly, filter out the data from `dates` table and then broadcast that filtered table onto `orders` table.\n",
    "* here, the spark will read only the part files that have the above data within it.\n",
    "* in this way minimum part files were read the output was desplayed.\n",
    "\n",
    ">> Condition to apply `Dynamic Partition Pruning`\n",
    ">> * Works for fast and dimension like tables. where one table and large and other table is small enough to broadcast.\n",
    ">> * Fact table must be partitioned. the data should be partiton on same column on which the join is done.\n",
    ">> * The small table should be broadcasted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When DPP not work??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the large data is not paritioned on key on which it is joining with other table.\n",
    "* both the tables that are joining are large and none of the table is broadcasted.\n",
    "* the data is not partitioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is data skewness problem??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assume you have a dataframe will some keys and there is one particular key that is in large quantity.\n",
    "* in this case, one of the parition will be large and other paritions will be small.\n",
    "* this is the data skweness problem.\n",
    "* here all other task will be executed, but one of the task will be very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What causes Data Skewness??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GroupBy operation.\n",
    "* skewed data from source\n",
    "* join operation\n",
    "* Inadequare partition strategy (bydefauly we use hash partitioning strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages or Cons of Data Skewness??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Slow Running Task\n",
    "* Spliting data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the ways to remove skewness??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* increasing shuffle partition\n",
    "* using repartition\n",
    "* custom partitioning\n",
    "* Broadcast table\n",
    "* AQE\n",
    "* Salting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is salting??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Salting is the technique through which we can remove skewness in the partitions.\n",
    "* we create a new column, that will help us to devide the data partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we implement salting??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Project Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure Related Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What cluster manager you have used in project??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have worked on yarn\n",
    "* we just need to mention `--master yarn` in configuration.\n",
    "* inside yarn we have queue, in my organisation we had 2TB cluster of ram which was assigned to our group.\n",
    "* the whole team together was using 2 TB of ram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the size of the cluster??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inside cluster we had queue, which was of 2 TB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source and Sink related question??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does your data comes into s3 bucket??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use a custom API that was built specifically to extract data from our various data sources. This custom API is designed to handle the specific requirements of our data extraction process, ensuring that the data is efficiently and reliably gathered.\n",
    "* data was ingested into source bucket by extracting it using API\n",
    "* Daily we use to get around 1TB of data from different vendors.\n",
    "* other team was responsible to mange this API and get data into S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the other sources you have used??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we used the dimension tables, perfrom some transformation and store them into s3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the sink in your project??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we have s3 bucket to store data.\n",
    "* we had a staging table where we write the logs of task whether it ran sucessfuly or failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the frequency of data in source??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* data was on daily basis.\n",
    "* it was a batch data\n",
    "* we use to receive data at 1 AM CST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In depth Question related project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the volumne that you deal with??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we use to get a lot of data from different vendors.\n",
    "* around 100 GB of data we use to collect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please explain your project in detail??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AIM: the aim of this project was to send out a daily list of customer data to marketing team, so that they can make calls.\n",
    "* we used to receive data into s3.\n",
    "* we used to read data into spark. it was a parquet file format.\n",
    "* we were suppose to process the data before 9 AM, and the data should be transfered before 9AM, because the daily work of marketing teaam was dependent on this list.\n",
    "* we were deling with the credit card data, and it is one o the popular US based company.\n",
    "* in between the process we have DQ check at the stand and end of the data processing that assure of Uniqueness, recency checks and schema check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the challenges you have faced and how did you resolved it??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the main challenge that i faced while creating this project was to understand the data and business logic behind it.\n",
    "* there were lot of different terms that were used to represent the states of the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which optimization technique you have used and why??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we used `caching`, because we used one dataframe that was used multiple time in transformation for joining with other dfs.\n",
    "* also done the `broadcast` of dimension table.\n",
    "* usd `AQE`\n",
    "* used best practice code (i.e. not used `select * statemet`) to extract data, only important data was extracted.\n",
    "* alos used `repartition` before storing data to s3.\n",
    "* tried avoiding `UDFs` and tried using built in spark function\n",
    "* leavraged `predicate pushdown` as early as possible to filter out data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How you done spark performance tuning??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adjust executor memory: provided enough memory to each executor, so that the executor run effeciently without failing.\n",
    "    * `--executor-memory 4G`\n",
    "* Driver memory: ensured effecient memory is provided to driver for storing metadata.\n",
    "    * `--driver-memory 4G`\n",
    "* parallelism: set the number of partition to optimum level to ensure tasks are evenly distributed.\n",
    "* Resource configuration: set the number of core and executors based on cluster configuration.\n",
    "    * `--num-executors 10`\n",
    "    * `--executor-cores 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### can you please walk me through the spark-submit command that you have used??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    spark-submit \\\n",
    "    --name SparkApp \\\n",
    "    --master yarn \\\n",
    "    --deploy-mode cluster \\\n",
    "    --executor-memory 4G \\\n",
    "    --driver-memory 4G \\\n",
    "    --num-executors 10 \\\n",
    "    --executor-cores 4 \\\n",
    "    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \\\n",
    "    --conf spark.sql.shuffle.partitions=200 \\\n",
    "    main.py\n",
    "\n",
    "* This is the trial and error method, where we used to change configs and test out the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the size of your team??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we had 4 developers.\n",
    "* 1 tech lead\n",
    "* 1 manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do you take your code to higher environment??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Firstly, we used to Test our code in `DEV` and `UAT`.\n",
    "* once everythin works fine, we used to raise a Pull request for the PROD and get it revied from the team mates.\n",
    "* further deployment was done by DevOps Team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling related question??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do you schedule your job in production??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we used the receive file on daily basis.\n",
    "* the files were lading in S3 in between 1 AM - 2 PM.\n",
    "* EDD functionality were buidl to trigger dag, once the faile was landed in bucket.\n",
    "* it was taking around 1 Hr to run the entite pipeline.\n",
    "* the file was delivered at around 3 - 4 AM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the frequency of your job??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* daily basis, the job was used to start at around 2 AM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How much time it takes to run a job??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Daily we used to get 200 GB of data, it use to take 1 hr to 1.30 hr to run the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do you notify business users if your job fails??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there is a Pagerduty softwarwe that we used to monitor our pipelines.\n",
    "* if the dag didnt start as expected time, we used to get the alert\n",
    "* also if any task fails we use to get a failure ticket for it on pagerduty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do you reprocess the data if job fails??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are mainy resons for a job failure.\n",
    "* one common reason was that the session was killed at run time.\n",
    "* in this situation we to recreate cluster and rerun the dag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if we missed to process a data from perticular day.\n",
    "* we used to catchup mode, that will pick the dated file we want to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Related Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Who is your client??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* i cant revil my client info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the impact of the project on the business??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the data was reached to market team for further calling on time.\n",
    "* which eventually helped in gaining more customer and increase revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the data domain??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* it was a credit card data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How long did it took to make this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* it took us around 4 months to complete the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
