{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EY Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Broadcasting in spark\n",
    "2. How does spark handle fault tolerance\n",
    "3. Why spark is preferred over MapReduce\n",
    "4. Two Scenarios where you had to do spark optimizations\n",
    "5. Significance of partitions in spark\n",
    "6. What is window function?\n",
    "7. group by vs window function which would you prefer\n",
    "8. Types of joins\n",
    "9. Other optimization techniques in spark\n",
    "10. AQE in spark\n",
    "11. pyspark code for getting the latest transaction for every user\n",
    "12. read a csv file in pyspark\n",
    "13. Various pyspark Dataframe operations code\n",
    "14. What is git add?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random LinkedIN Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  One of the frequently questions asked in the interviews I gave - How to optimize a Spark job? \n",
    "According to me these are the below points which I have answered : \n",
    "\n",
    "1. Select only the required columns for that specification. Prod table can have 100 to 1000 columns but all the columns are not required. \"Select *\" is a bad practice. \n",
    "\n",
    "2. Apply the filter at the beginning while reading the input data. All historical data may not be required each time. Possibly the input table is partitioned and we need only the latest partition to process in the target. \n",
    "\n",
    "3. Analyse the input data. Maybe the volume of the data distributed unevenly in the files that have been read. Now there are two options either we can 'coalesce' or 'repartition'. I prefer to reparation the data. Coalesce merges the files that are there in the same executor and it can create data skewness. While doing a reparation, if we know the key by which it will be joined in the next step, it is advisable to do reparation with the key as well that the file size comes around 128mb. \n",
    "\n",
    "4. The target table possibly partitioned. While saving the process data, don't use the saveAsTable API, use insertInto API. It is much faster while writing a large number of data and decreasing the execution time. \n",
    "\n",
    "5. If a small dataframe is being used multiple times in different parts of the code, cache it. \n",
    "\n",
    "6. Run some statistics based on the joining key and if the data is skewed, enable AQE. It is one of the powerful features that comes with Spark 3.x and it has many other benefits also. I will write a separate post on it. \n",
    "\n",
    "7. Try to avoid unnecessary wide transformation. Also avoid cross join. Left join or inner join can be the choice but it depends on the use case. \n",
    "\n",
    "8. If there is a small table that has been used to join, broadcast it. Keep one executor per node cause this small data will be copied in each executor. Choose graviton EC2 instance for the EMR cluster according to the input data so that the number of executors is equal to the number of nodes. \n",
    "\n",
    "9. By default spark.sql partitions is 200, but modify the number of it based on the input volume of the data. \n",
    "\n",
    "10. Keep or select the driver node in such a way that the memory of it is more than the final output. Also, submit production jobs in cluster mode instead of client mode. In cluster mode we don't need to keep track of the driver node. \n",
    "\n",
    "11. The key by which the join will perform, check if there is NULL for that key. If we don't require NULL key value rows, we can filter out those rows. Else, we can separate it from the join and later do union of that dataframe with the joined dataframe as those NULL keys will not take part in the join. We just need to add columns with NULL values in it before union.\n",
    "\n",
    "12. Always check the execution plan/DAG. Maybe there are unnecessary repetitive steps that can be optimized. \n",
    "\n",
    "Note- Please add any other optimization techniques that I'm missing and let me know if you have any other thoughts for any of the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➤ How do you deploy PySpark applications in a production environment?  \n",
    "➤ What are some best practices for monitoring and logging PySpark jobs?  \n",
    "➤ How do you manage resources and scheduling in a PySpark application?  \n",
    "➤ Write a PySpark job to perform a specific data processing task (e.g., filtering data, aggregating results).  \n",
    "➤ You have a dataset containing user activity logs with missing values and inconsistent data types. Describe how you would clean and standardize this dataset using PySpark.  \n",
    "➤ Given a dataset with nested JSON structures, how would you flatten it into a tabular format using PySpark?  \n",
    "➤ Your PySpark job is running slower than expected due to data skew. Explain how you would identify and address this issue.  \n",
    "➤ You need to join two large datasets, but the join operation is causing out-of-memory errors. What strategies would you use to optimize this join?  \n",
    "➤ Describe how you would set up a real-time data pipeline using PySpark and Kafka to process streaming data.  \n",
    "➤ You are tasked with processing real-time sensor data to detect anomalies. Explain the steps you would take to implement this using PySpark.  \n",
    "➤ Describe how you would design and implement an ETL pipeline in PySpark to extract data from an RDBMS, transform it, and load it into a data warehouse.  \n",
    "➤ Given a requirement to process and transform data from multiple sources (e.g., CSV, JSON, and Parquet files), how would you handle this in a PySpark job?  \n",
    "➤ You need to integrate data from an external API into your PySpark pipeline. Explain how you would achieve this.  \n",
    "➤ Describe how you would use PySpark to join data from a Hive table and a Kafka stream.  \n",
    "➤ You need to integrate data from an external API into your PySpark pipeline. Explain how you would achieve this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.sql import StructType, StructField, InegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructFiled('Id', IntegerType()),\n",
    "    StructField('name', StringType())\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
